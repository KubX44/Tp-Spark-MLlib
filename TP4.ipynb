{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMu0JHWH3r+w0uhUxqGLgI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KubX44/Tp-Spark-MLlib/blob/main/TP4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Stocker le contenu de tous les fichiers du dossier tp4_data dans un DataFrame."
      ],
      "metadata": {
        "id": "S9_ORjC2GvAG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Me5G-1K9G1NC",
        "outputId": "5b7c2013-a09b-4a54-c19e-23d439d9755f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=fd70446e66a57259145350ba43cf89556b823cc727a1aae47fcb2b4f32c77d12\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"tp4\").getOrCreate()"
      ],
      "metadata": {
        "id": "Re6O-fTtHEr5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DataFrame = spark.read.format(\"csv\")\\\n",
        ".option(\"header\", \"true\")\\\n",
        ".option(\"inferSchema\", \"true\")\\\n",
        ".load(\"/content/*.csv\")"
      ],
      "metadata": {
        "id": "KLF64l49HLO8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Afficher le schéma du Dataframe obtenu\n"
      ],
      "metadata": {
        "id": "ZUPqUz_jHSfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DataFrame.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAlcqzcfHYaT",
        "outputId": "f131814a-103d-472d-c033-95f11dbc5f6e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- InvoiceNo: string (nullable = true)\n",
            " |-- StockCode: string (nullable = true)\n",
            " |-- Description: string (nullable = true)\n",
            " |-- Quantity: integer (nullable = true)\n",
            " |-- InvoiceDate: timestamp (nullable = true)\n",
            " |-- UnitPrice: double (nullable = true)\n",
            " |-- CustomerID: double (nullable = true)\n",
            " |-- Country: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Remplir les valeurs manquantes (NaN) avec la valeur 0.\n"
      ],
      "metadata": {
        "id": "OmY1KQacHicc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DataFrame_filled = DataFrame.fillna(0)\n",
        "DataFrame_filled.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r38MajCjHkdp",
        "outputId": "c73fdc1b-f2eb-495c-a895-3d03f0dc8591"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
            "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
            "|   537226|    22811|SET OF 6 T-LIGHTS...|       6|2010-12-06 08:34:00|     2.95|   15987.0|United Kingdom|\n",
            "|   537226|    21713|CITRONELLA CANDLE...|       8|2010-12-06 08:34:00|      2.1|   15987.0|United Kingdom|\n",
            "|   537226|    22927|GREEN GIANT GARDE...|       2|2010-12-06 08:34:00|     5.95|   15987.0|United Kingdom|\n",
            "|   537226|    20802|SMALL GLASS SUNDA...|       6|2010-12-06 08:34:00|     1.65|   15987.0|United Kingdom|\n",
            "|   537226|    22052|VINTAGE CARAVAN G...|      25|2010-12-06 08:34:00|     0.42|   15987.0|United Kingdom|\n",
            "|   537226|    22705|   WRAP GREEN PEARS |      25|2010-12-06 08:34:00|     0.42|   15987.0|United Kingdom|\n",
            "|   537226|    20781|GOLD EAR MUFF HEA...|       2|2010-12-06 08:34:00|     5.49|   15987.0|United Kingdom|\n",
            "|   537226|    22310|IVORY KNITTED MUG...|       6|2010-12-06 08:34:00|     1.65|   15987.0|United Kingdom|\n",
            "|   537226|    22389|PAPERWEIGHT SAVE ...|       6|2010-12-06 08:34:00|     2.55|   15987.0|United Kingdom|\n",
            "|   537227|    22941|CHRISTMAS LIGHTS ...|       2|2010-12-06 08:42:00|      8.5|   17677.0|United Kingdom|\n",
            "|   537227|    22696| WICKER WREATH LARGE|       6|2010-12-06 08:42:00|     1.95|   17677.0|United Kingdom|\n",
            "|   537227|    22193|RED DINER WALL CLOCK|       2|2010-12-06 08:42:00|      8.5|   17677.0|United Kingdom|\n",
            "|   537227|    21212|PACK OF 72 RETROS...|     120|2010-12-06 08:42:00|     0.42|   17677.0|United Kingdom|\n",
            "|   537227|    21977|PACK OF 60 PINK P...|      48|2010-12-06 08:42:00|     0.55|   17677.0|United Kingdom|\n",
            "|   537227|    84991|60 TEATIME FAIRY ...|      48|2010-12-06 08:42:00|     0.55|   17677.0|United Kingdom|\n",
            "|   537227|    21213|PACK OF 72 SKULL ...|      48|2010-12-06 08:42:00|     0.55|   17677.0|United Kingdom|\n",
            "|   537227|    21080|SET/20 RED RETROS...|      12|2010-12-06 08:42:00|     0.85|   17677.0|United Kingdom|\n",
            "|   537227|    22632|HAND WARMER RED R...|      48|2010-12-06 08:42:00|      2.1|   17677.0|United Kingdom|\n",
            "|   537227|    22315|200 RED + WHITE B...|      12|2010-12-06 08:42:00|     1.25|   17677.0|United Kingdom|\n",
            "|   537227|    21232|STRAWBERRY CERAMI...|      12|2010-12-06 08:42:00|     1.25|   17677.0|United Kingdom|\n",
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) Ajouter une nouvelle colonne nommée \"day_of_week\". La valeur de la colonne est le\n",
        "jour de la semaine correspondant à la date de chaque ligne dans la colonne\n",
        "\"InvoiceDate\"."
      ],
      "metadata": {
        "id": "4Syx9R3IHwG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import date_format, col\n",
        "preppedDataFrame = DataFrame\\\n",
        ".na.fill(0)\\\n",
        ".withColumn(\"day_of_week\", date_format(col(\"InvoiceDate\"), \"EEEE\"))\\\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "trRgvzUcHx_P"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5) Diviser les données en un ensemble d'apprentissage et un ensemble de test. Effectuer\n",
        "la division en se basant sur l'attribut InvoiceDate : l'ensemble d'apprentissage contient\n",
        "les achats effectués avant 2010-12-13 et l'ensemble de test contient les achats effectués\n",
        "durant ou après 2010-12-13.\n"
      ],
      "metadata": {
        "id": "v1BK7QQkJn3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainDataFrame = preppedDataFrame\\\n",
        ".where(\"InvoiceDate < '2010-12-13'\")\n",
        "testDataFrame = preppedDataFrame\\\n",
        ".where(\"InvoiceDate >= '2010-12-13'\")"
      ],
      "metadata": {
        "id": "w93iyGlIJom_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6) Créer un StringIndexer qui permet de transformer les jours de semaine présents dans\n",
        "la colonne day_of_week en valeurs numériques correspondantes.\n"
      ],
      "metadata": {
        "id": "nD570mEEJxxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "indexer = StringIndexer()\\\n",
        ".setInputCol(\"day_of_week\")\\\n",
        ".setOutputCol(\"day_of_week_index\")"
      ],
      "metadata": {
        "id": "GXF_3QdXJzBe"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7) En utilisant le StringIndexer, Spark peut par exemple représenter samedi par 6 et\n",
        "lundi par 1. Cependant, avec ce schéma de numérotation, nous indiquons\n",
        "implicitement que samedi est supérieur à lundi (par des valeurs numériques pures).\n",
        "Ceci est évidemment incorrect. Comment résoudre ce problème ?\n"
      ],
      "metadata": {
        "id": "oSFDOTDZKCjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import OneHotEncoder\n",
        "encoder = OneHotEncoder()\\\n",
        ".setInputCol(\"day_of_week_index\")\\\n",
        ".setOutputCol(\"day_of_week_encoded\")"
      ],
      "metadata": {
        "id": "va10tReJKE5I"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8) Créer un VectorAssembler contenant trois attributs : UnitPrice, Quantity, et\n",
        "day_of_week_encoded."
      ],
      "metadata": {
        "id": "MJplq2fnKNfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "vectorAssembler = VectorAssembler()\\\n",
        ".setInputCols([\"UnitPrice\", \"Quantity\", \"day_of_week_encoded\"])\\\n",
        ".setOutputCol(\"features\")"
      ],
      "metadata": {
        "id": "lQ-iexFCKP__"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9) Créer un pipeline configuré avec les résultats des étapes 6, 7 et 8."
      ],
      "metadata": {
        "id": "soO5I_ZkKUd_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "transformationPipeline = Pipeline()\\\n",
        ".setStages([indexer, encoder, vectorAssembler])"
      ],
      "metadata": {
        "id": "b2lihVxGKXO9"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10) Notre StringIndexer doit savoir combien de valeurs uniques il y a à indexer, comment\n",
        "résoudre ce problème ?"
      ],
      "metadata": {
        "id": "H8JoFSxGKgRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fittedPipeline = transformationPipeline.fit(trainDataFrame)"
      ],
      "metadata": {
        "id": "uSqlSemLKhNl"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11) Transformer les données de l’ensemble d’apprentissage en se basant sur les étapes\n",
        "(stages) du pipeline."
      ],
      "metadata": {
        "id": "JR5dfSAqKm8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformedTraining = fittedPipeline.transform(trainDataFrame)"
      ],
      "metadata": {
        "id": "NWLxk2QwKp5w"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12)Créer une instance de KMeans, on supposse que le nombre de clusters est 20"
      ],
      "metadata": {
        "id": "e7d6_m6JK8cy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.clustering import KMeans\n",
        "kmeans = KMeans()\\\n",
        ".setK(20)"
      ],
      "metadata": {
        "id": "GkZ6fHifK9AN"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "13) Lancer l’apprentissage de KMeans en se basant sur l’ensemble obtenu dans l’étape 11."
      ],
      "metadata": {
        "id": "o00hMnqBLoLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kmModel = kmeans.fit(transformedTraining)"
      ],
      "metadata": {
        "id": "-xB3JM9uLrHl"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "14) Effectuer des prédictions sur l’ensemble de test."
      ],
      "metadata": {
        "id": "hBh2KvDGLzAG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformedTest = fittedPipeline.transform(testDataFrame)\n",
        "predictions = kmModel.transform(transformedTest)"
      ],
      "metadata": {
        "id": "VlCxpULmL1ZV"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "15)Calculer le coefficient de silhouette."
      ],
      "metadata": {
        "id": "d_1Zal8DL9VO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "evaluator = ClusteringEvaluator()\n",
        "silhouette = evaluator.evaluate(predictions)"
      ],
      "metadata": {
        "id": "uIRmP5-EL-Ws"
      },
      "execution_count": 23,
      "outputs": []
    }
  ]
}